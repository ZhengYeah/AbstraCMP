题目：基于多路径回溯的神经网络验证方法



## 摘要

针对神经网络验证问题，本文提出多路径回溯的概念。我们发现已有的方法均采用一条回溯路径计算每个神经网络节点的上下界，是这一概念的特例。使用多条回溯路径可以有效改善这类方法的精度。我们在一些通用数据集上将我们的方法与目前已有工具定量比较，结果表明了这种方法的有效性。

## 前言

（说明问题）

神经网络已经广泛应用于各种现实场景。过去难以处理的高维复杂问题，如图像分类，目前一定程度上可以达到人类的分类水平。随着硬件和算法的进步，甚至神经网络模型在一些问题上已经十分成熟，但同时其可解释性却十分有限。另一方面，由于自身不可避免的过拟合以及攻击技术的发展，使得无论来自于自然或人为影响，神经网络都是极其容易受到攻击的（引用）。因此在一些安全攸关的应用场景下，神经网络的部署应用存在较强的局限性。比如对于自动驾驶系统，路标的识别错误可能会导致灾难性的后果。若将神经网络应用于这样的场景，其鲁棒性必须有严格的保证。

提高鲁棒性的方法可以是尽量训练泛化能力较强的网络，定量衡量一个神经网络的泛化能力需要在训练完成后使用足够多的测试案例来测试，但有限的测试用例并不能保证其绝对安全性。如一张 28 × 28 像素的灰阶图片，对其施加 $3$ 个像素的扰动，得到的图片就有 $\tbinom{28 \times 28}{3} \times 2^{3 \times 8}$ 种，全部测试显然是不现实的。面对这样的问题，神经网络的形式化验证受到了广泛关注，它能够为神经网络的安全性提供数学保证。

神经网络验证要处理的典型问题是一个网络的可达性质，即给定一个输入范围，确定经过这个网络输出能够到达的范围。在实际问题中，比如图像分类网络，这一性质通常和局部鲁棒性联系在一起，即对于一张具体的输入图片和一个允许的最大扰动范围，经过一个神经网络得到的分类标签不会发生改变，这时我们称这个神经网络关于输入图片和扰动范围是鲁棒的，反之若存在分类标签的变化，则称这个网络关于给定输入和扰动范围是不鲁棒的。这一问题的困难之处在于神经网络中非线性激活函数的叠加，导致整个网络模型难以准确求解。（引用）的综述中介绍了目前神经网络验证的主流方法和工具，同时能够验证的网络规模也在逐年增加，然而遗憾的是目前依然没有足够有效的方法能够直接应用于工业场景的问题规模，因此，发展更加快速和准确的神经网络验证方法备受关注，也是本篇文章的关注点。

（简单介绍我们的方法）

本篇文章提出一种新的基于多路径回溯的方法来验证前馈神经网络。多路径回溯是指在计算一个节点取值的上下界时，在多种抽象方式得到的多条符号路径上回溯，在符号路径上回溯的目的是得到这个节点取值关于输入的符号表示，然后将回溯得到的数值上下界取交集作为这个节点的数值上下界。相比于已有的在一条符号路径上回溯的方法，这种方法能够得到一个节点更精确的取值范围。另一方面本文的方法能够保持符号路径回溯方法的低时间复杂度，相对于需要使用线性规划求解的方法，更适用于较大规模网络的验证。

（相关工作）

根据得到验证结果的强度不同，目前的验证方法大致可以分为两类。一类方法提供可靠且完备的结果，即我们不仅能够相信它给出的安全答案，也能够相信它给出的不安全答案，显然这需要得到整个网络在给定输入范围下的准确可达集。对于 ReLU 网络，这类方法通常需要对激活状态不确定的节点分情况讨论，分成关于这个节点的两个线性函数。依据实现技术差异这类方法可大致分为四组：（1）基于 SMT，在实现上将验证问题编码为可满足性问题来求解（引用 Reluplex， Planet）；（2）基于 MILP，在实现上将验证问题编码为 MILP 问题，节点的激活与不激活分别对应整数 1 和 0，从而可以使用 Gurobi 等支持 MILP 的求解器求解（引用 bounded milp）；（3）基于 BaB，在实现上根据各种启发式策略来确定分割哪个激活状态不确定的节点（引用 BABSR），目前也有工作使用图学习来指导分割（引用）；（4）基于抽象精化（引用 ReluVal），通过反复分割输入域构成子问题，分别验证这些容易验证的子问题直到能够得到准确的验证结果或者超时。前三组基于 SMT，MILP 和 BaB 的方法本质上都是对激活状态不确定的 ReLU 节点分割，所以最坏情况下需要描绘出一个神经网络能够确定的 $2^{N}$ 个线性函数，其中 $N$ 为节点个数。虽然有很多高效的启发式策略来缓解这一复杂度，但目前对于大规模网络的验证仍然是不现实的。

对于 ReLU 网络，另一类方法提供可靠却不完备的结果，即能够相信它给出的安全答案，但不能相信它给出的不安全答案。这类方法通常对整个网络的可达集做上近似处理，即真正的可达集包含于求得的可达集。因此虽然这类方法得到的可达集与不安全区域有交集，并不能说明网络真正的可达集与不安全区域有交集。这类方法大致分为两组：（1）基于线性抽象（引用 AI2，DeepPoly），使用线性抽象将 ReLU 激活函数抽象为包含其凸包的线性约束区域；（2）基于半正定规划（引用），（补充）。这类方法相对于求出准确可达集的方法有较低的时间复杂度，通常越精确的抽象方法对真正可达集的近似程度越高，当然越精确的抽象方法时间代价也相对越高，（引用 Fast-lin）给出对于计算一定范围输入下网络可达集这一问题，除非 $\text{P} = \text{NP}$，否则不存在近似率为 $(1 - o(1))\ln N$ 的多项式时间算法，其中 $N$ 为网络节点数量。

本文属于线性抽象方法的工作，旨在提高这类方法的精度。值得注意的是，本篇文章主要关注激活函数为 ReLU 的神经网络的验证问题，但我们的方法不仅适用于 ReLU 激活函数，它同样适用于其他类型的激活函数，如 tanh 和 sigmoid 等。此外，如（引用 RefineZono）中所做，我们的方法同样可以作为一个通用的框架，来结合代价较高但更精确的方法，如 LP 以及 MILP 等。 



## 贡献

* 我们提出了一种新的神经网络验证方法，这种方法能够结合多种 ReLU 激活函数线性抽象方式的优势，从而提高目前基于线性抽象的神经网络验证方法的精度；
* 我们的方法有较强的通用性，除 ReLU 激活函数之外，它同样适用于各种类型的激活函数，另一方面我们的方法也能够结合一些更为精确但代价稍高的方法，从而平衡精度与速度。



## 背景

（前馈神经网络）

本篇文章主要关注激活函数为 ReLU 的前馈神经网络的验证问题。前馈神经网络是节点分层表示的有向无环图，第一层称为输入层，最后一层称为输出层，我们用 $x_{i, j}$ 表示第 $i$ 层的第 $j$ 个节点，为了避免引入不必要的符号，在不会产生歧义的上下文中也用 $x_{i, j}$ 表示对应节点的值。除了 $i = 1$ 即输入层外，每个节点 $x_{i, j}$ 被它的所有上层节点用单向边连接，这些单向边的权重构成节点 $x_{i,j}$ 的权重向量 $W_{i, j}$，除权重向量外每个 $x_{i, j}$ 有一个常量偏移 $b_{i, j}$。为了描述方便，本篇文章将 ReLU 层作为一种层类型加到每个具有 ReLU 激活函数的仿射变换层节点之后，形成仿射变换层和 ReLU 层交替的神经网络结构（如图 X）。这样一个 ReLU 层节点 $x_{i + 1, j}$ 与连接它的仿射变换层节点 $x_{i, j}$ 的关系是 $x_{i + 1, j} = ReLU(x_{i, j}) \overset{\Delta}{=} \max(0, x_{i, j})$，其中 $x_{i, j}$ 由连接它的所有 ReLU 层节点构成的值向量 $\mathbf{x}_{i - 1}$ 经过仿射变换 $x_{i, j} = W_{i, j}\mathbf{x}_{i - 1} + b_{i, j}$ 得到。



![](C:\Users\Ye Zheng\Desktop\SVG\figure_1.svg)



给定一个有界范围的输入，每个节点 $x_{i, j}$ 的取值不再是一个数值而是一个数值区间 $[l_{i, j}, u_{i, j}]$，其中 $l_{i, j}$ 称为节点 $x_{i, j}$ 数值下界，$u_{i, j}$ 称为节点 $x_{i, j}$ 的数值上界。如果 $x_{i, j}$ 是仿射变换层节点且 $l_{i, j} < 0 < u_{i, j}$，称这个节点激活状态不确定。

一个神经网络确定了一个从输入到输出的函数 $f$，通常 $f$ 没有显式表达且从经验上来看它具有十分复杂的映射关系，因此验证问题最关注的是 $f$ 的可达集性质。 

定义：（验证问题）给定神经网络 $f: \mathbb{R}^{d_1} \to \mathbb{R}^{d_k}$，其中 $d_{1}$ 表示输入层的维数，$d_{k}$ 表示输出层的维数；给定输入域 $\mathcal{X}_{1}$，不安全输出范围$\bar{S}$。验证问题的目标是确定是否有：对于 $\forall x_{1} \in \mathcal{X}_{1}$， $f(x_1) \cap \bar{S} = \emptyset$。

如果可达集 $f(\mathcal{X}_{1})$ 与不安全区域 $\bar{S}$ 交集为空，称神经网络 $f$ 关于性质 $(\mathcal{X}_{1}, \bar{S})$ 是安全的，反之若交集非空，则关于性质$(\mathcal{X}_{1}, \bar{S})$ 是不安全的。例如图像分类问题中，输入域 $\mathcal{X}_{1}$ 是对一张原始图片经过一定大小扰动的全体图片，不安全区域 $\bar{S}$ 可以是除原始图片的标签之外的所有标签，这样上述验证问题即图像分类网络的局部鲁棒性（引用王小维），验证 $f(\mathcal{X}_{1})$ 与 $\bar{S}$ 没有交集等价于验证正确标签对应的节点值恒大于其他标签对应的节点值。事实上不安全区域一般都由输出层线性表出，因此验证问题的关键实际上是求出输出层节点的可达集。对于线性抽象等不完备方法，关键是如何更准确地近似输出层节点的真正可达集。

（线性抽象）

本篇文章基于线性抽象的验证方法，线性抽象将非线性激活函数 ReLU 抽象为包含其凸包的线性约束区域，因为处理线性函数的叠加比处理非线性函数叠加简单很多。图 X 表示常见的四种 ReLU 抽象方式。图中的横轴表示一个仿射变换层节点 $x_{i, j}$ 的取值，它具有数值上下界 $[l_{i, j}, u_{i, j}]$，纵轴表示它经过 ReLU 抽象的取值。



![](C:\Users\Ye Zheng\Desktop\SVG\abstraction_methods.svg)



图（a）通常称为 LP 松弛或 LP 抽象，（引用 Planet）其抽象区域重合于 ReLU 激活函数的二维凸包，可以表述为以下线性约束
$$
\begin{align*}
x_{i + 1, j} &\leq \frac{u_{i, j}}{u_{i, j} - l_{i, j}}(x_{i, j} - l_{i, j}) \\
x_{i + 1, j} &\geq x_{i, j} \\
x_{i + 1, j} &\geq 0.
\end{align*}
$$
在单纯对一个 ReLU 激活函数的线性抽象上，这种抽象方法是最精确的。（引用）将 ReLU 激活函数和仿射变换一同编码，能够比 LP 抽象更精确。在 LP 抽象中一个重要的问题是每个节点的下界约束有两个，这使得确定这个节点的数值上下界实际上成为一个最优化问题，这也是其名称 LP 抽象的来源。使用 LP 抽象计算每个节点的数值上下界能够得到较精确的结果，但随着网络深度和宽度增加，接近输出层的节点上下界对应的 LP 问题会变得十分复杂。

图（b）（c）（d）分别称为大直角三角形抽象、大钝角三角形抽象及平行四边形抽象（引用）。这三种抽象方法中，ReLU 节点的上界和下界约束分别只有一个，这使得可以逐层地传递上下界，而不涉及复杂的优化方法，是神经网络验证中速度很快的方法。

下面的定理 X 说明这四种对 ReLU 抽象的可靠性。

定理：给定 $l_{i, j} < 0, u_{i, j} > 0, \forall \lambda \in [0,1]$，对于 $\forall x_{i, j} \in [l_{i, j}, u_{i, j}]$ 成立 $\lambda x_{i, j} \leq ReLU(x_{i, j}) \leq \frac{u_{i, j}}{u_{i, j} - l_{i, j}} (x_{i, j} - l_{i, j})$。

*证明：注意到 $\frac{u}{u - l} \in (0, 1)$ 且 $\hat{z} - l \geq 0$，由 ReLU 激活函数的定义，当 $\hat{z} < 0$ 时，$ReLU(\hat{z}) = 0$，上式显然成立；当 $\hat{z} \geq 0$ 时， 因 $\frac{u}{u - l} (\hat{z} - l) - \hat{z} = \frac{(\hat{z} - u)}{u - l}l \geq 0$，故上式成立。*

当 $\lambda = 0$ 以及 $\lambda = 1$ 时，即 LP 抽象，当 $\lambda = \frac{u_{i, j}}{u_{i, j} - l_{i, j}}$ 时即平行四边形抽象。此外定理 X 还表明，固定上界 $\frac{u_{i, j}}{u_{i, j} - l_{i, j}}(x_{i, j} - l_{i, j})$，任意 $\lambda x_{i, j}$ 都可以作为 $ReLU(x_{i, j})$ 的下界，从而构成一种仅有一个上界约束和一个下界约束的 ReLU 可靠抽象。

（DeepPoly）

DeepPoly 使用简单的线性抽象来计算一个神经网络每个节点的数值上下界，对于每个节点，它维护由上层节点表示此节点的符号约束，然后通过这些符号约束向输入层逐层回溯，最终得到的这个节点的数值上下界。用上层节点符号约束此节点可以刻画仿射变换以及 ReLU 函数的抽象，回溯到输入层可以最大化保留节点取值的依赖信息。

下图例子表示 DeepPoly 计算每个节点数值上下界的过程，图中各节点偏移都为 0，权重在前向边上标出，如 $x_{41}$ 经过 $x_{31}$ 和 $x_{32}$ 仿射变换得到，它们之间的关系是 $x_{31} - x_{32} \leq x_{41} \leq x_{31} - x_{32}$，这是 $x_{41}$ 取值的符号约束。为了得到 $x_{41}$ 的数值上下界，需要有 $x_{31}$ 以及 $x_{32}$ 的数值上下界，因此需要向输入层回溯，$x_{31}$ 由 $x_{21}$ 经过 ReLU 函数得到，这里我们使用大直角三角形抽象，因此有 $0 \leq x_{31}$ 以及 $x_{31} \leq 0.5x_{21} + 1$，同理，对于 $x_{32}$ 有 $0 \leq x_{32}$ 以及 $x_{32} \leq 0.5x_{22} + 1$，那么 $x_{41}$ 的上界是 $x_{41} \leq 0.5x_{21} + 1 - 0$，下界是 $x_{41} \geq 0 - (0.5x_{22} + 1)$，按照同样的方法继续回溯，最终可以得到 $x_{41}$ 用输入层 $x_{11}$ 和 $x_{12}$ 表示的符号约束为 $-0.5x_{11} + 0.5x_{12} - 1 \leq x_{7} \leq -0.5x_{11} + 0.5x_{12} + 1$，通过代入$x_{11}$ 和 $x_{12}$ 的最值得到 $-2 \leq x_{41} \leq 2$，作为 $x_{41}$ 的数值上下界。



![](C:\Users\Ye Zheng\Desktop\SVG\figure_2.svg)



对于所有的节点，DeepPoly 都会用这种回溯到输入层的方法来求得其数值上下界。

除了回溯到输入层之外，DeepPoly 还会应用一个有效的启发式策略来减少对每个 ReLU 激活函数的抽象误差：对于一个激活状态不确定的 ReLU 节点，比如 $x_{41}$，如果求得其数值上下界为 $l_{41}$ 和 $u_{41}$，若 $|l_{41}| \leq u_{41}$，则选择大直角三角形抽象，因为此时大直角三角形的面积为 $\frac{1}{2} \cdot u_{41}(u_{41} - l_{41})$ ，小于大钝角三角形的面积 $\frac{1}{2} \cdot |l_{41}|(u_{41} - l_{41})$；若 |$l_{41}| > u_{41}$，则选择大钝角三角形抽象，因为此时大钝角三角形的面积小于大直角三角形。

DeepPoly 得到每个节点的数值上下界关于原 ReLU 网络是可靠的，即对于每个节点，它在原 ReLU 网络中的数值上下界包含于 DeepPoly 得到的数值上下界。因此通过这种方法实际上得到原网络 $f$ 的一个上近似网络 $\hat{f}$，任意输入在 $\hat{f}$ 下的可达集是其在 $f$ 下可达集的上近似。这意味着可以利用 $\hat{f}$ 快速得到 $f$ 的一个安全边界，如果一个输入在 $\hat{f}$ 下是安全的，那么蕴含它在 $f$ 下是安全的。

在下一节，我们会介绍一种新的方法，它能够明显改善对 $f$ 上近似的精度，而只引入较小的额外时间消耗。



## 方法

尽管 DeepPoly 使用回溯来维持节点之间取值的依赖关系，并且使用启发式策略来减少对 ReLU 的抽象误差，但它实际上只使用一条路径的符号约束回溯，为了说明这一点，同样可以看它计算 $x_{41}$ 的数值上界的过程：$x_{41}$ 上界的符号约束是 $x_{31} - x_{32}$，然后分别代入 $x_{31}$ 的符号约束上界 $x_{31} \leq 0.5x_{21} + 1$ 以及 $x_{6}$ 的符号约束下界 $x_{32} \geq 0$，得到 $x_{41} \leq 0.5x_{21} + 1 - 0$，然后代入 $x_{21}$ 的符号约束上界，最终得到 $x_{41} \leq -0.5x_{11} + 0.5x_{12} + 1$。整个过程对每个符号变量进行一次代换，得到 $x_{41}$ 用输入层表示的符号上界约束也只有一个。

如果能够得到对于 $x_{41}$ 的多个符号上界约束，那么就有可能得到 $x_{41}$ 更准确的数值上界，这是本篇文章的主要想法。我们使用多种对 ReLU 的可靠抽象构成多条回溯路径，从而对于一个节点，可以得到多个用输入层表示的符号上界约束，以得到更准确的数值上界。更具体地，（1）我们维护多种 ReLU 抽象来传递符号约束，在计算每个节点数值上下界时分别使用这个节点的多种符号约束向输入层回溯，（2）得到的多个数值上下界的交集作为这个节点的数值上下界。通过维护多条回溯路径，我们可以得到每个节点更多的上下界信息。



![](C:\Users\Ye Zheng\Desktop\SVG\figure_3.svg)



图 X 说明了具体的步骤，它是与图 X 相同的网络。例如对于 $x_{31}$，我们不仅维护其经过大直角三角形抽象的约束 $0 \leq x_{31}$ 及 $ x_{31} \leq 0.5x_{22} + 1$，也维护其经过平行四边形抽象的约束 $0.5x_{21} \leq x_{31}$ 及 $x_{31} \leq 0.5x_{21} + 1$，然后向输入层回溯，分别得到 $x_{31}$ 的数值上下界为 $[0, 2]$ 和 $[-1, 2]$。我们取这两个上下界的交集作为 $x_{31}$ 的数值上下界。按照同样的做法，我们会取 $x_{41}$ 的数值上下界为 $[-2, 2]$，并且用它来构造对 $ReLU(x_{41})$ 的两种抽象。在 $x_{61}$ 处，我们发现，这种方法已经比 DeepPoly 更好，它得到 $x_{12}$ 的数值上下界为 $[-1, 1]$，而 DeepPoly 得到 $[-2, 2]$，因为对于 $x_{61}$ 的数值上界，DeepPoly 有
$$
\begin{align*}
x_{61} &\leq x_{51} - x_{52} \\
&\leq 0 + 0.5x_{8} + 1 \\
&\leq 0.5(x_{5} - x_{6}) + 1 \\
&\leq 0.5(0.5x_{3} + 1 + 0) + 1 = 0.25x_{3} + 1.5 \\
&\leq 0.25(-x_1 + x_{2}) + 1.5 \\
&\leq 2.
\end{align*}
$$
我们的方法除了进行上述的回溯路径外，还会使用另外一条回溯路径计算
$$
\begin{align*}
x_{12} &\leq -x_{9} + x_{10} \\
&\leq -0.5x_{7} + 0.5x_{8} + 1 \\
&\leq -0.5(x_{5} - x_{6}) + 0.5(x_{5} - x_{6}) + 1 \\
&= 1
\end{align*}
$$
并最终取 $x_{12}$ 的数值上界为两者的较小值 $1$。同理对于 $x_{12}$ 的下界，也可以得到比 DeepPoly 更紧的界。更紧的界是我们的首要目标，因为它还可以用来构造更精确的抽象，从而更准确地近似真正的可达集。事实上，对于上图的网络，用 MILP 可求得 $x_{12}$ 的准确数值上下界为 $[0, 0]$。

对于每一个激活状态不确定的节点，我们选择不同抽象方法得到的数值上下界的交集作为它的数值上下界，定理 X 说明了这种做法的可行性。

（定理及证明）



算法 1 给出了上述方法的伪代码，算法输入为网络 $f$ 和其输入范围 $\mathcal{X}_{1}$，算法输出为 $f$ 的输出层节点的数值上下界。因为我们使用仿射变换和 ReLU 分为两层表示，所以需要根据不同的层类型来表示 $x_{i, j}$，在第 X 行， 如果第 $i$ 层为仿射变换层，那么用 $i - 1$ 层节点仿射变换表示 $x_{i, j}$，这里 $W_{i, j}$ 和 $b_{i, j}$ 表示节点 $x_{i, j}$ 的权重矩阵和偏移，$\mathbf{x}_{i - 1}$ 表示 $i - 1$ 层所有节点值构成的向量。在第 X 行，如果第 $i$ 层为 ReLU 层，那么 $x_{i, j}$ 应该为 $x_{i - 1, j}$ 经过 ReLU 抽象的结果，我们根据 $M$ 种线性抽象方式来用 $x_{i - 1, j}$ 线性表示 $x_{i, j}$，即通过选用不同的 $\lambda_{m}$ 来实现。现在我们有了 $x_{i, j}$ 关于 $i - 1$ 层节点的符号约束，可以按照 X 节的方法回溯到输入层的 $\mathcal{X}_{0}$ 来得到节点 $x_{i, j}$ 的 $M$ 个数值上下界，如第 X 行，这里我们省略了回溯的具体过程。第 X 行取 $M$ 个数值上下界的交集作为节点 $x_{i, j}$ 的数值上下界 $[l_{i, j}, u_{i, j}]$，根据定理 X，这 $M$ 个数值上下界关于原 ReLU 网络都是可靠的，所以它们的交集也是可靠的。 更精确的 $[l_{i,j}, u_{i, j}]$ 被用来构造下一层的 ReLU 抽象，以减少误差积累。最后算法返回输出层每个节点的数值上下界。



```pseudocode
Algorithm 1: Compute output numerical bounds
function AbstraCMP(f, X_{1})
	Input: 网络 f，输入范围 X_{1}，Output: 输出层每个节点的数值上下界
	for i <- 1 to I do
		for j <- 1 to J_{i} do
			if layer_type[i] == AFFINE then
				x_{i, j} = W_{i, j}x_{i - 1} + b_{i, j}
			if layer_type[i] == RELU then
				for m = 1 to M do
					x_{i, j} <= ____
					x_{i, j} >= \lambda_{m} x_{i - 1, j}
			for m = 1 to M do
				[l_{i, j, m}, u_{i, j, m}] <- BACK_PROPAGATE(x_{i, j}, m)
			[l_{i, j}, u_{i, j}] = \cap [l_{i, j, m}, u_{i, j, m}]
	return [l_{I}, u_{I}]    // 返回输出层每个节点的数值上下界
```



算法 1 继承了符号路径回溯方法的高效性，其时间复杂度是 $\mathcal{O}(N^2)$，其中 $N$ 为输入网络的节点数量，对于每个节点，回溯计算其数值上下界代价是 $\mathcal{O}(N)$。$M$ 为回溯路径的数量，一般取低值常数。空间复杂度是 $\mathcal{O}(N)$，对每个节点保存常数项个约束。

理论上回溯路径的数量可以任意多，即 $M $ 可以任意大。那么一个值得关心的问题是，任意多的回溯路径能够达到多好的精度？在 X 节我们测试了随 $M$ 增加，精度的改善效果。



## 实验

我们将第 X 节的方法实现为工具 AbstraCMP，实现语言为 Python 3.9，源代码可以在 X 获取。在本节中我们将对 AbstraCMP 的表现相对于 DeepPoly 作定量比较。

同绝大部分工作一样，我们关注于对一个具体输入的无穷范数扰动，它由无穷范数距离（或称为切比雪夫距离）定义。

（无穷范数距离定义）给定两个 $n$ 维向量 $x_{1} = (a_{i}), z_{1} = (b_{i})$，其中$1 \leq i \leq n$，它们的无穷范数距离 $d_{\infty}(x_{1}, z_{1})$ 定义为 $d_{\infty}(x_{1}, z_{1}) = \underset{1 \leq i \leq n}{\max}|a_{i} - b_{i}|$。

通常输入范围 $\mathcal{X}_{1}$ 由上述度量给出，即给定具体输入 $x_{1}$ 和扰动大小 $\delta$， $\mathcal{X}_{1} = \{z_{1} | d_{\infty}(x_{1}, z_{1}) \leq \delta\}$。直观而言 $\mathcal{X}_{1}$ 的元素是 $x_{1}$ 每个维度最大加或减 $\delta$ 构成的向量。

对于分类网络的验证问题，鲁棒半径能够很好地量化比较不同上近似方法的精度。

（鲁棒半径定义）对于一个网络 $f$，给定具体输入 $x_{1}$ 和对应的不安全区域$\bar{S}$，$f$ 关于 $(x_{1}, \bar{S})$ 的鲁棒半径 $R_{f}(x_{1}, \bar{S}) = \max d_{\infty}(x_1, z_1)$，其中 $f(z_{1}) \cap \bar{S} = \emptyset$。

对于分类网络，$\bar{S}$ 通常是 $x_{1}$ 真实标签以外的所有标签。因此直观而言，网络 $f$ 关于 $x_{1}$ 的鲁棒半径就是能对 $x_{1}$ 施加的最大扰动 $\delta$ 而不遇到对抗样本。若 $\hat{f}_{1}$ 和 $\hat{f}_{2}$ 是对 $f$ 的两个上近似网络，对于同一个输入 $x_{1}$，若 $R_{\hat{f}_{1}}(x_{1}, \bar{S}) > R_{\hat{f}_{2}}(x_{1}, \bar{S})$，则 $\hat{f}_{1}$ 关于 $(x_{1}, \bar{S})$ 比 $\hat{f}_{2}$ 更准确地近似 $f$。因此比较两种上近似方法的精确程度，只需要比较他们针对一些输入的鲁棒半径大小。

（引用 ACAS Xu）用于无人机设备的防碰撞系统。由 45 个 ReLU 前馈神经网络组成，这 45 个网络规模相同，均为 $6 \times 50$，其中 $6$ 表示隐藏层层数，$50$ 表示每个隐藏层节点个数。每个网络输入层和输出层均是 $5$ 维，是低维输入网络的代表。

（引用 MNIST）是包含 $0 \sim 9$ 十个手写数字图像的数据集，包含 6 万个训练样本以及 1 万个测试样本。每个样本都是灰阶图像且大小均为 $28 \times 28$。我们使用 MNIST 数据集训练了规模分别为 $16 \times 50$ 和 $8 \times 100$ 的两个全连接前馈 ReLU 网络，每个网络的输入层为 $784$ 维，输出层为 $10$ 维，是较高维输入的代表。

（硬件和软件参数）

实验平台参数为，处理器 Intel Core i7-6700 @ 3.40GHz，内存大小 16GB，操作系统版本 Windows 10 专业版 64 位，Python 版本 3.9。以下实验均在相同环境下进行。



### 1 低维输入网络上的精度改善

首先，对于 ACAS Xu 我们随机选择四个输入和它们对应的决策构成四个性质。然后测试 ACAS Xu 的 45 个网络关于这四个性质的鲁棒半径。在本实验中我们采用 X 节的启发式策略，同时为了避免过多回溯路径带来的时间代价，仅使用定理 X 中 $\alpha = 0$ 和 $\alpha = 1$ 两条回溯路径， 即 $M = 1$。得到结果如图 X 所示。（解释图）

![](C:\Users\Ye Zheng\Desktop\SVG\experiment_1.svg)

如图一表示性质 1 在 45 个网络下的鲁棒半径，其中蓝色为 AbstraCMP 求得的鲁棒半径，橙色为 DeepPoly 求得的鲁棒半径。对于任意输入和任意网络，AbstraCMP 可以保证至少有 DeepPoly 的效果。在实验中，对于合法的随机输入数据，AbstraCMP 在 45 个网络上得到的平均鲁棒半径相对于 DeepPoly 都能够提升 20% ~ 30%。

### 2 回溯路径数量的影响

在上一节提到，理论上回溯路径可以任意多，在下面的实验中，我们将展示回溯路径的增加对所得鲁棒半径的改进。我们固定一个 ACAS Xu 网络，并固定一个性质，然后测试在取不同的回溯路径数量时，所能得到的鲁棒半径以及对应的时间消耗。关于回溯路径的选择，因为定理 X 中每个 $\alpha \in [0, 1]$ 都构成对 ReLU 激活函数的可靠抽象，所以我们选取包含 0 和 1 的 $M$ 等分值构成 $M + 1$ 条回溯路径。也就是说，若 $M = 4$，则 $\alpha = \{0, 0.25, 0.5, 0.75, 1 \}$ 共 5 个值构成 5 条回溯路径。随 $M$ 值变化得到的鲁棒半径变化如图 X，对应的时间消耗如表 X。（解释图和表）

（图）

![](C:\Users\Ye Zheng\Desktop\SVG\experiment_2.svg)

（表）

![image-20210701163219723](C:\Users\Ye Zheng\AppData\Roaming\Typora\typora-user-images\image-20210701163219723.png)

从图 X 可以看出，增加较少的回溯路径就能取得相对于 DeepPoly 很好的效果。在本实验的网络和输入下，仅使用 $M = 1$ 确定的 DeepPoly 路径，$\alpha = 0$ 路径和$\alpha = 1$ 路径三条回溯路径就能获得相对于 DeepPoly 的明显提升，随 $M$ 增加可以得到更精确的结果，但这种改善并不能持续，在我们的实验中，发现对于几乎所有用例，$M = 3$ 就已经能够取得收敛值；对于大部分用例，通常 $M = 1$ 或 $M = 2$ 就已经接近收敛值。

表 X 给出了随回溯路径数量增加，时间的代价变化，以及与 DeepPoly 的对比。对于每一列的时间消耗，我们记录其运行 10 次的时间，然后取平均值，如 DeepPoly 的用时 6.43 秒是在 10 次运行的平均时间消耗。如 X 节的讨论，AbstraCMP 的时间消耗与回溯路径数量 $M$ 线性相关，且几乎是 DeepPoly 时间代价的 $M$ 倍。值得注意的是，我们使用简单的串行实现，但这种方法是可以高度并行的。对于 DeepPoly，它可以并行计算同层的节点数值上下界，对于 AbstraCMP，除同层不同节点之外，同一节点不同回溯路径的数值上下界也可以并行化计算，以充分利用 GPU 的计算能力。

### 3 高维输入网络上的精度改善

最后我们测试了 AbstraCMP 在高维输入网络上的表现，我们使用 MNIST 数据集训练了两个全连接前馈神经网络，记为 MNIST 8×100 和 MNIST 16×50，即它们的隐藏层规模分别为 8 × 100 和 16 × 50。然后我们从 MNIST 数据集的前 100 张图片中随机选取 20 张，测试它们在两种规模网络下能够得到的鲁棒半径，本实验中使用启发式策略及 $\alpha = 0$ 和 $\alpha = 1$ 两条回溯路径。得到结果如图 X 所示。



（解释图）



## 结论

 本篇文章

（需要加最新的相关课题工作）如 autoLiRPA



另外我准备把多路径回溯的概念作为贡献的一点，因为之前没有人提到
